---
layout: post
title: "Robots.io"
date: 2015-01-22 15:05:03
comments: true
description: "Java library designed to make parsing a websites 'robots.txt' file easy."
keywords: "Java, library, robots.io"
category: project
projectlinks: 
  github : "https://github.com/JamesFrost/robots.io"
  javadoc : "http://robotsio.sourceforge.net/"
tags:
- project
- program
---

<script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js?skin=desert"></script>

<p>Java library designed to make parsing a websites 'robots.txt' file easy.</p>

<img src="{{ site.url | append: '/assets/robotsio/robotsmall.png'}}" class="container hero" />

<h2>Usage</h2>
<p>The <a href="https://github.com/JamesFrost/robots.io/blob/master/src/me/jamesfrost/robotsio/RobotsParser.java">RobotsParser</a> class provides all the functionality to use robots.io.</p>

<h2>Examples</h2>
<h3>Connecting</h3>

<p>To parse the robots.txt for Google with the User-Agent string "test":</p>

<pre class="prettyprint">
	RobotsParser robotsParser = new RobotsParser("test");
	robotsParser.connect("http://google.com");
</pre>
<br>
<p>Alternatively, to parse with no User-Agent, simply leave the constructor blank.</p>

<p>You can also pass a domain with a path.</p>

<pre class="prettyprint">
	robotsParser.connect("http://google.com/example.htm"); //This would also be valid
</pre>
<br>
<p>Note: Domains can either be passed in string form or as a <a href="http://docs.oracle.com/javase/7/docs/api/java/net/URL.html">URL</a> object to all methods.</p>

<h3>Querying</h3>
<p>To check if a URL is allowed:</p>

<pre class="prettyprint">
	robotsParser.isAllowed("http://google.com/test"); //Returns true if allowed
</pre>
<br>
<p>Or, to get all the rules parsed from the file:</p>

<pre class="prettyprint">
	robotsParser.getDisallowedPaths(); //This will return an ArrayList of Strings
</pre>
<br>
<p>The results parsed are cached in the robotsParser object until the connect() method is called again, overwriting the previously parsed data</p>

<h3>Politeness</h3>
In the event that all access is denied, a RobotsDisallowedException will be thrown.

<h3>URL Normalisation</h3>

<p>Domains passed to RobotsParser are normalised to always end in a forward slash. Disallowed Paths returned will never begin with a forward slash. This is so that URL's can easily be constructed. For example:</p>

<pre class="prettyprint">
	robotsParser.getDomain() + robotsParser.getDisallowedPaths().get(0);
</pre>
<br>
<p>Would return http://google.com/example.htm.</p>

<h3>Licensing</h3>
Robots.io is distributed under the <a hred="https://github.com/JamesFrost/robots.io/blob/master/LICENSE">GPL v3</a>.
